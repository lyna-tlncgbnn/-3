{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc85ed1a-d7d9-4c4f-a5eb-f42b7652645b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-20T01:17:26.922858Z",
     "iopub.status.busy": "2021-11-20T01:17:26.921976Z",
     "iopub.status.idle": "2021-11-20T01:17:28.542055Z",
     "shell.execute_reply": "2021-11-20T01:17:28.541323Z",
     "shell.execute_reply.started": "2021-11-20T01:17:26.922813Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import h5py\n",
    "import matplotlib_inline\n",
    "from paddle.io import DataLoader\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "import paddle.nn.functional as F\n",
    "import copy\n",
    "from Tsception_data_process import  PrepareData\n",
    "from TSception import TSception\n",
    "from MSBAM import MSBAM\n",
    "from MSBAM_data_process import  DataDel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa655e1-3aa4-4c81-84f9-e0817c2a4832",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-20T01:17:28.616873Z",
     "iopub.status.busy": "2021-11-20T01:17:28.616648Z",
     "iopub.status.idle": "2021-11-20T01:17:28.647857Z",
     "shell.execute_reply": "2021-11-20T01:17:28.647361Z",
     "shell.execute_reply.started": "2021-11-20T01:17:28.616847Z"
    }
   },
   "outputs": [],
   "source": [
    "class process:\n",
    "    def __init__(self,model = 1):\n",
    "        self.model = model\n",
    "    def train_one_epoch(self,data_loader, net , loss_func, optimizer):\n",
    "        net.train()\n",
    "        floss = 0\n",
    "        for i, (x_batch, y_batch) in enumerate(data_loader()):\n",
    "            out = net(x_batch)\n",
    "            loss = loss_func(out, y_batch)\n",
    "        #    avloss = paddle.mean(loss)\n",
    "            floss = floss+loss\n",
    "        #    _, pred = paddle.max(out, 1)\n",
    "        #    pred_train.extend(pred.data.tolist())\n",
    "        #    act_train.extend(y_batch.data.tolist())  #tolist() 返回列表或者数字\n",
    "            optimizer.clear_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        #floss = floss/i+1\n",
    "        return floss\n",
    "\n",
    "    def get_model(self,num_classes=2,input_size=(1,28,512),sampling_rate=128,num_T=15,num_S=15,hidden=32,dropout_rate=0.8):\n",
    "        if self.model == 1:\n",
    "\n",
    "            model = TSception(\n",
    "                num_classes=num_classes, input_size=input_size,\n",
    "                sampling_rate=sampling_rate, num_T=num_T, num_S=num_S,\n",
    "                hidden=hidden, dropout_rate=dropout_rate)\n",
    "        else:\n",
    "            model = MSBAM(2)\n",
    "        return model\n",
    "\n",
    "    def split_balance_class(self,data,label, k = 8, random = True):\n",
    "        np.random.seed(888)\n",
    "        KF = KFold(n_splits=k,shuffle=True)\n",
    "        for idx_fold, (index_train, index_test) in enumerate(KF.split(data)):\n",
    "            train_index,test_index = index_train,index_test\n",
    "            break\n",
    "        train = data[train_index]\n",
    "        train_label = label[train_index]\n",
    "        val = data[test_index]\n",
    "        val_label = label[test_index]\n",
    "        return train, train_label, val, val_label\n",
    "\n",
    "    def normalize(self,train, test):\n",
    "\n",
    "        \"\"\"\n",
    "            this function do standard normalization for EEG channel by channel\n",
    "            :param train: training data\n",
    "            :param test: testing data\n",
    "            :return: normalized training and testing data\n",
    "        \"\"\"\n",
    "        # data: sample x 1 x channel x data\n",
    "        mean = 0\n",
    "        std = 0\n",
    "        for channel in range(train.shape[2]):\n",
    "            mean = np.mean(train[:, :, channel, :])\n",
    "            std = np.std(train[:, :, channel, :])\n",
    "            train[:, :, channel, :] = (train[:, :, channel, :] - mean) / std\n",
    "            test[:, :, channel, :] = (test[:, :, channel, :] - mean) / std\n",
    "        return train, test\n",
    "\n",
    "\n",
    "    def train(self,data_train, label_train, data_val, label_val,epochs = 100,batch_size = 64,save_path = None,crosv = False,k = None):\n",
    "\n",
    "        CUDA = True\n",
    "        loss_list = []\n",
    "        #   save_name = '_sub' + str(subject) + '_trial' + str(fold)\n",
    "        #   set_up(args)\n",
    "        #    train_dataset = MyDataset(data_train,label_train)\n",
    "        train_loader = self.get_dataloader(data_train,label_train,shuffle = True,batch_size=batch_size)\n",
    "\n",
    "        #    val_dataset = MyDataset(data_val,label_val)\n",
    "        val_loader = self.get_dataloader(data_val,label_val,shuffle = True,batch_size = batch_size)\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        model = self.get_model()\n",
    "        # para = get_trainable_parameter_num(model)\n",
    "        # print('Model {} size:{}'.format(args.model, para))\n",
    "        #可以省略\n",
    "\n",
    "        if CUDA:\n",
    "            paddle.set_device('gpu') if paddle.is_compiled_with_cuda() else paddle.set_device('cpu')\n",
    "        if self.model == 1:\n",
    "            lr = 0.001\n",
    "        else:\n",
    "            lr = 0.0001\n",
    "        optimizer = paddle.optimizer.Adam(learning_rate=lr,parameters = model.parameters())\n",
    "\n",
    "        for epoch in range(1, epochs+1):\n",
    "\n",
    "            loss = self.train_one_epoch(data_loader=train_loader, net=model,loss_func = loss_fn, optimizer=optimizer)\n",
    "            loss_list.append(loss.numpy())\n",
    "    #    acc_train, f1_train, _ = get_metrics(y_pred=pred_train, y_true=act_train)\n",
    "    #    print('epoch {}, loss={:.4f} acc={:.4f} f1={:.4f}'\n",
    "    #          .format(epoch, loss_train, acc_train, f1_train))\n",
    "\n",
    "    #    loss_val, pred_val, act_val = predict(\n",
    "    #        data_loader=val_loader, net=model, loss_fn=loss_fn\n",
    "    #    )\n",
    "    #    acc_val, f1_val, _ = get_metrics(y_pred=pred_val, y_true=act_val)\n",
    "    #    print('epoch {}, val, loss={:.4f} acc={:.4f} f1={:.4f}'.\n",
    "    #          format(epoch, loss_val, acc_val, f1_val))\n",
    "            print('epoch: {} --- loss:{}'.format(epoch,loss.numpy()))\n",
    "            act,pre,ls = self.predict(val_loader,model,loss_fn)   #ls = loss\n",
    "            acc,f1,_ = self.get_metrics(pre,act)\n",
    "            print('  val: loss:{}  acc:{}  f1:{}'.format(ls.numpy(),acc,f1))\n",
    "            if epoch % 100 ==0 and crosv == False:\n",
    "                paddle.save(model.state_dict(),'./models-MSBAM/model_epoch_{}.pdparams'.format(epoch))\n",
    "        if crosv ==False:\n",
    "            paddle.save(model.state_dict(),'./models-MSBAM/model_final.pdparams')\n",
    "        else:\n",
    "            paddle.save(model.state_dict(),save_path+'/model_{}_fold.pdparams'.format(k))\n",
    "\n",
    "#data_loader = paddle.io.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    def predict(self,data_loader, net,loss_func):\n",
    "        net.eval()\n",
    "        avloss = 0\n",
    "        pred_val = []\n",
    "        act_val = []\n",
    "        with paddle.no_grad():\n",
    "            for i, data in enumerate(data_loader):\n",
    "                out = net(data[0])\n",
    "                loss =loss_func(out, data[1])\n",
    "                out = F.softmax(out)\n",
    "                pred = paddle.argmax(out, 1)\n",
    "                avloss = avloss + loss\n",
    "                pred_val.extend(pred.numpy())\n",
    "                act_val.extend(data[1].numpy())\n",
    "        return  act_val,pred_val,avloss/i+1\n",
    "\n",
    "    def test(self,data, label, model_path ,batch_size=32,k=None,crosv = False):\n",
    "        CUDA = True\n",
    "        test_loader = self.get_dataloader(data, label,batch_size = batch_size, shuffle=False)\n",
    "        model = self.get_model()\n",
    "        if crosv:\n",
    "            path = model_path+'/model_{}_fold.pdparams'.format(k)\n",
    "        else:\n",
    "            path = model_path\n",
    "        model.load_dict(paddle.load(path))\n",
    "    #CUDA = torch.cuda.is_available()  #自己添加的 TQ\n",
    "        if CUDA:\n",
    "            paddle.set_device('gpu') if paddle.is_compiled_with_cuda() else paddle.set_device('cpu')\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        \"\"\"\n",
    "        if reproduce:\n",
    "            model_name_reproduce = 'sub' + str(subject) + '_fold' + str(fold) + '.pth'\n",
    "            data_type = 'model_{}_{}_{}'.format(args.dataset, args.data_format, args.label_type)\n",
    "            save_path = osp.join(args.save_path, data_type)\n",
    "            ensure_path(save_path)\n",
    "            model_name_reproduce = osp.join(save_path, model_name_reproduce)\n",
    "            model.load_state_dict(torch.load(model_name_reproduce,map_location=torch.device('cpu')))\n",
    "    \n",
    "        else:\n",
    "            model.load_state_dict(torch.load(args.load_path))\n",
    "        loss, pred, act = predict(\n",
    "            data_loader=test_loader, net=model, loss_fn=loss_fn\n",
    "        )\n",
    "        \"\"\"\n",
    "        act,pred,loss = self.predict(test_loader,model,loss_fn)\n",
    "        acc, f1, _ = self.get_metrics(y_pred=pred, y_true=act)\n",
    "        print('>>> Test:  loss={:.4f} acc={:.4f} f1={:.4f}'.format(loss.item(), acc, f1))\n",
    "        return act, pred, loss\n",
    "    def trial_wise_voting(self,act, pred, num_segment_per_trial = 15):\n",
    "        \"\"\"\n",
    "        this function do voting within each tiral to get the label of entire trial\n",
    "        :param act: [num_sample] list\n",
    "        :param pred: [num_sample] list\n",
    "        :param num_segment_per_trial: how many samples per trial\n",
    "        :param trial_in_fold: how many trials in this fold\n",
    "        :return: trial-wise actual label and predicted label\n",
    "        \"\"\"\n",
    "        trial_in_fold = len(pred)/num_segment_per_trial\n",
    "        num_trial = int(len(act)/num_segment_per_trial)\n",
    "        assert num_trial == trial_in_fold\n",
    "        act_trial = np.reshape(act, (num_trial, num_segment_per_trial))   #4*15\n",
    "        pred_trial = np.reshape(pred, (num_trial, num_segment_per_trial))\n",
    "        #print(\"\\n\", \"act:\", act_trial)\n",
    "        act_trial = np.mean(act_trial, axis=-1).tolist()\n",
    "        pred_vote = []\n",
    "        for trial in pred_trial:    #for each row\n",
    "            index_0 = np.where(trial == 0)[0]\n",
    "            index_1 = np.where(trial == 1)[0]\n",
    "            if len(index_1) >= len(index_0):\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            pred_vote.append(label)\n",
    "        #print(\"\\n\",\"act_trial:\",act_trial,'\\npred_vote:',pred_vote)\n",
    "        return act_trial, pred_vote\n",
    "\n",
    "\n",
    "    def get_metrics(self,y_pred, y_true, classes=None):\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        if classes is not None:\n",
    "            cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "        else:\n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "        return acc, f1, cm\n",
    "\n",
    "    def get_dataloader(self,data,label,shuffle,batch_size):\n",
    "        dataset = MyDataset(data,label)\n",
    "        loader = DataLoader(dataset,shuffle = shuffle,batch_size=batch_size)\n",
    "        return loader\n",
    "    def prepare_data(self,idx_train, idx_test, data, label):\n",
    "        \"\"\"\n",
    "            1. get training and testing data according to the index\n",
    "            2. numpy.array-->torch.tensor\n",
    "            :param idx_train: index of training data\n",
    "            :param idx_test: index of testing data\n",
    "            :param data: (trial, segments, 1, channel, data)\n",
    "            :param label: (trial, segments,)\n",
    "            :return: data and label\n",
    "        \"\"\"\n",
    "        data_train = data[idx_train]\n",
    "        label_train = label[idx_train]\n",
    "        data_test = data[idx_test]\n",
    "        label_test = label[idx_test]\n",
    "\n",
    "        data_train = np.concatenate(data_train, axis=0)\n",
    "        label_train = np.concatenate(label_train, axis=0)\n",
    "\n",
    "            # the testing data do not need to be concatenated, when doing leave-one-trial-out\n",
    "        if len(data_test.shape)>4:\n",
    "            data_test = np.concatenate(data_test, axis=0)\n",
    "            label_test = np.concatenate(label_test, axis=0)\n",
    "    \n",
    "#if MSBAM dont need this -------!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        if self.model == 1:\n",
    "            data_train, data_test = normalize(train=data_train, test=data_test)\n",
    "\n",
    "        # Prepare the data format for training the model\n",
    "        #data_train = torch.from_numpy(data_train).float()\n",
    "        #abel_train = torch.from_numpy(label_train).long()\n",
    "\n",
    "        #data_test = torch.from_numpy(data_test).float()\n",
    "        #label_test = torch.from_numpy(label_test).long()\n",
    "        return data_train, label_train, data_test, label_test\n",
    "\n",
    "class MyDataset(paddle.io.Dataset):\n",
    "    def __init__(self,data,label):\n",
    "        self.x = data\n",
    "        self.y = label\n",
    "        assert self.x.shape[0] == self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f090f893-f460-44fb-9a01-8fc9735badd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-20T01:17:28.985611Z",
     "iopub.status.busy": "2021-11-20T01:17:28.985207Z",
     "iopub.status.idle": "2021-11-20T01:17:28.993948Z",
     "shell.execute_reply": "2021-11-20T01:17:28.993457Z",
     "shell.execute_reply.started": "2021-11-20T01:17:28.985586Z"
    }
   },
   "outputs": [],
   "source": [
    "def log2txt(filename,content):\n",
    "    \"\"\"\n",
    "    this function log the content to results.txt\n",
    "    :param content: string, the content to log\n",
    "    \"\"\"\n",
    "    file = open(filename, 'a')\n",
    "    file.write(str(content) + '\\n')\n",
    "    file.close()\n",
    "def corss_validation(VV,LL,data_path,model,Kfold = 10,depend = True,train = True,save_path = None,):\n",
    "    if model == 1:\n",
    "        log_file = \"TSception_results.txt\"\n",
    "    else:\n",
    "        log_file = \"MSBAM_results.txt\"\n",
    "    file = open(log_file, 'a')\n",
    "    file.write(\"---------results----------\")\n",
    "    file.close\n",
    "    proce = process(model = model)\n",
    "    \"\"\"\n",
    "    if model == 1:\n",
    "        predata = PrepareData()\n",
    "        VV,LL = predata.together(data_path)\n",
    "    else:\n",
    "        predata = DataDel()\n",
    "        VV,LL = predata.getdata(data_path)\n",
    "    \"\"\"\n",
    "    VV = VV.astype('float32')\n",
    "    LL = LL.astype('int')\n",
    "    np.random.seed(666)\n",
    "    kf = KFold(n_splits=Kfold ,shuffle=True)\n",
    "    for idx_fold, (index_train, index_test) in enumerate(kf.split(VV)):\n",
    "        V,L,data_test,label_test = proce.prepare_data(index_train,index_test,VV,LL)\n",
    "        V,L,val,val_l = proce.split_balance_class(V,L,k=8)\n",
    "        if train:\n",
    "            proce.train(V,L,val,val_l,epochs=150,crosv=True,save_path=save_path,k = idx_fold+1)\n",
    "        act,pred,loss = proce.test(data_test,label_test,model_path = save_path,k = idx_fold+1,crosv=True)\n",
    "        acc,f1,_ = proce.get_metrics(pred,act)  #pred true\n",
    "        content = '{}_fold:  acc:{}  f1:{}'.format(idx_fold+1,acc,f1)\n",
    "        log2txt(log_file,content)\n",
    "        if depend:\n",
    "            if model == 1:\n",
    "                seg = 15\n",
    "            else:\n",
    "                seg = 12\n",
    "            trial_act,trial_pred = proce.trial_wise_voting(act,pred,seg)  #act, pred, num_segment_per_trial \n",
    "            trial_acc,trial_f1,_ = proce.get_metrics(trial_pred,trial_act)\n",
    "            content = '{}_fold:  trial_acc:{}  trial_f1:{} \\n'.format(idx_fold+1,trial_acc,trial_f1)\n",
    "            print(content)\n",
    "            log2txt(log_file,content)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "378411d3-59e1-4ca4-b1ae-58964cb075da",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-11-20T01:19:42.635540Z",
     "iopub.status.busy": "2021-11-20T01:19:42.635059Z",
     "iopub.status.idle": "2021-11-20T01:22:19.279586Z",
     "shell.execute_reply": "2021-11-20T01:22:19.278446Z",
     "shell.execute_reply.started": "2021-11-20T01:19:42.635498Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1120 09:19:53.540599   562 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\n",
      "W1120 09:19:53.545145   562 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 --- loss:[401.03143]\n",
      "  val: loss:[1.8524892]  acc:0.5416666666666666  f1:0.667785234899329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 --- loss:[381.9377]\n",
      "  val: loss:[1.8133776]  acc:0.5381944444444444  f1:0.6661087866108787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 --- loss:[340.12115]\n",
      "  val: loss:[1.7907305]  acc:0.5370370370370371  f1:0.6669442131557036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 --- loss:[315.1444]\n",
      "  val: loss:[1.7719383]  acc:0.5480324074074074  f1:0.6787330316742082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 --- loss:[296.9502]\n",
      "  val: loss:[1.7602701]  acc:0.5532407407407407  f1:0.6897106109324759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 --- loss:[277.64407]\n",
      "  val: loss:[1.7579997]  acc:0.5555555555555556  f1:0.6913183279742765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 --- loss:[260.5379]\n",
      "  val: loss:[1.7425532]  acc:0.5549768518518519  f1:0.6872712484749899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 --- loss:[250.65637]\n",
      "  val: loss:[1.732152]  acc:0.5578703703703703  f1:0.6866283839212469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 --- loss:[229.69954]\n",
      "  val: loss:[1.7262111]  acc:0.5665509259259259  f1:0.701949860724234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 --- loss:[222.40126]\n",
      "  val: loss:[1.7203166]  acc:0.5711805555555556  f1:0.7022900763358779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 --- loss:[214.10301]\n",
      "  val: loss:[1.7178171]  acc:0.5734953703703703  f1:0.7031816351188079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 --- loss:[199.9915]\n",
      "  val: loss:[1.719619]  acc:0.5798611111111112  f1:0.7091346153846154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 --- loss:[200.91685]\n",
      "  val: loss:[1.714485]  acc:0.5850694444444444  f1:0.7084180561203741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 --- loss:[189.57465]\n",
      "  val: loss:[1.711288]  acc:0.5833333333333334  f1:0.7009966777408637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 --- loss:[185.59071]\n",
      "  val: loss:[1.7101222]  acc:0.5902777777777778  f1:0.7079207920792078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 --- loss:[180.7455]\n",
      "  val: loss:[1.7083565]  acc:0.5856481481481481  f1:0.7055921052631579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 --- loss:[173.36946]\n",
      "  val: loss:[1.7041032]  acc:0.5844907407407407  f1:0.699581589958159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 --- loss:[170.60855]\n",
      "  val: loss:[1.7003131]  acc:0.59375  f1:0.7125307125307125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 --- loss:[165.6615]\n",
      "  val: loss:[1.700494]  acc:0.5920138888888888  f1:0.7051442910915935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 --- loss:[164.56876]\n",
      "  val: loss:[1.6972437]  acc:0.5931712962962963  f1:0.70793518903199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 --- loss:[161.15039]\n",
      "  val: loss:[1.6945732]  acc:0.5995370370370371  f1:0.7123857024106401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 --- loss:[159.87468]\n",
      "  val: loss:[1.6919625]  acc:0.6059027777777778  f1:0.7196377109921779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 --- loss:[152.2246]\n",
      "  val: loss:[1.6890421]  acc:0.6105324074074074  f1:0.721094073767095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 --- loss:[148.8473]\n",
      "  val: loss:[1.6869483]  acc:0.6082175925925926  f1:0.7201322860686235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 --- loss:[148.73637]\n",
      "  val: loss:[1.6851771]  acc:0.6168981481481481  f1:0.71733561058924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 --- loss:[147.21408]\n",
      "  val: loss:[1.6814905]  acc:0.6307870370370371  f1:0.7238095238095237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 --- loss:[145.51137]\n",
      "  val: loss:[1.6791654]  acc:0.6359953703703703  f1:0.7230295024218405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 --- loss:[142.03044]\n",
      "  val: loss:[1.6747274]  acc:0.6348379629629629  f1:0.7201773835920177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 --- loss:[139.87111]\n",
      "  val: loss:[1.667811]  acc:0.6429398148148148  f1:0.7273530711444985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 --- loss:[138.45457]\n",
      "  val: loss:[1.6665031]  acc:0.6446759259259259  f1:0.7283185840707964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 31 --- loss:[137.2665]\n",
      "  val: loss:[1.6613358]  acc:0.6597222222222222  f1:0.743231441048035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32 --- loss:[135.89622]\n",
      "  val: loss:[1.65316]  acc:0.6603009259259259  f1:0.7392270102176811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33 --- loss:[nan]\n",
      "  val: loss:[nan]  acc:0.4253472222222222  f1:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 34 --- loss:[nan]\n",
      "  val: loss:[nan]  acc:0.4253472222222222  f1:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35 --- loss:[nan]\n",
      "  val: loss:[nan]  acc:0.4253472222222222  f1:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 36 --- loss:[nan]\n",
      "  val: loss:[nan]  acc:0.4253472222222222  f1:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 37 --- loss:[nan]\n",
      "  val: loss:[nan]  acc:0.4253472222222222  f1:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 38 --- loss:[nan]\n",
      "  val: loss:[nan]  acc:0.4253472222222222  f1:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 39 --- loss:[nan]\n",
      "  val: loss:[nan]  acc:0.4253472222222222  f1:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 --- loss:[nan]\n",
      "  val: loss:[nan]  acc:0.4253472222222222  f1:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 41 --- loss:[nan]\n",
      "  val: loss:[nan]  acc:0.4253472222222222  f1:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 42 --- loss:[nan]\n",
      "  val: loss:[nan]  acc:0.4253472222222222  f1:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 43 --- loss:[nan]\n",
      "  val: loss:[nan]  acc:0.4253472222222222  f1:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 44 --- loss:[nan]\n",
      "  val: loss:[nan]  acc:0.4253472222222222  f1:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 45 --- loss:[nan]\n",
      "  val: loss:[nan]  acc:0.4253472222222222  f1:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46 --- loss:[nan]\n",
      "  val: loss:[nan]  acc:0.4253472222222222  f1:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 47 --- loss:[nan]\n",
      "  val: loss:[nan]  acc:0.4253472222222222  f1:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 48 --- loss:[nan]\n",
      "  val: loss:[nan]  acc:0.4253472222222222  f1:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49 --- loss:[nan]\n",
      "  val: loss:[nan]  acc:0.4253472222222222  f1:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50 --- loss:[nan]\n",
      "  val: loss:[nan]  acc:0.4253472222222222  f1:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 51 --- loss:[nan]\n",
      "  val: loss:[nan]  acc:0.4253472222222222  f1:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 52 --- loss:[nan]\n",
      "  val: loss:[nan]  acc:0.4253472222222222  f1:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 53 --- loss:[nan]\n",
      "  val: loss:[nan]  acc:0.4253472222222222  f1:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 54 --- loss:[nan]\n",
      "  val: loss:[nan]  acc:0.4253472222222222  f1:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_562/2408787292.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorss_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'DATA_MSBAM_DEAP_A/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'MSBAM_K_fold'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_562/2079424191.py\u001b[0m in \u001b[0;36mcorss_validation\u001b[0;34m(VV, LL, data_path, model, Kfold, depend, train, save_path)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_balance_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mproce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_l\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcrosv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx_fold\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx_fold\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcrosv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#pred true\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_562/4273671504.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_train, label_train, data_val, label_val, epochs, batch_size, save_path, crosv, k)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mloss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m#    acc_train, f1_train, _ = get_metrics(y_pred=pred_train, y_true=act_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_562/4273671504.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(self, data_loader, net, loss_func, optimizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#    act_train.extend(y_batch.data.tolist())  #tolist() 返回列表或者数字\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#floss = floss/i+1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-253>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_tensor, retain_graph)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/wrapped_decorator.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mwrapped_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m         assert in_dygraph_mode(\n\u001b[1;32m    228\u001b[0m         ), \"We only support '%s()' in dynamic graph mode, please call 'paddle.disable_static()' to enter dynamic graph mode.\" % func.__name__\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/varbase_patch_methods.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_tensor, retain_graph)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 core.dygraph_run_backward([self], [grad_tensor], retain_graph,\n\u001b[0;32m--> 249\u001b[0;31m                                           framework._dygraph_tracer())\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corss_validation(VV,LL,'DATA_MSBAM_DEAP_A/',model=2,save_path='MSBAM_K_fold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a37e3d5-57e6-4356-87f2-1a26e798486d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-20T01:17:31.713363Z",
     "iopub.status.busy": "2021-11-20T01:17:31.712498Z",
     "iopub.status.idle": "2021-11-20T01:19:29.533806Z",
     "shell.execute_reply": "2021-11-20T01:19:29.533076Z",
     "shell.execute_reply.started": "2021-11-20T01:17:31.713320Z"
    }
   },
   "outputs": [],
   "source": [
    "predata = DataDel()\n",
    "VV,LL = predata.getdata('DATA_MSBAM_DEAP_A/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80c7cf8c-0872-450e-805b-1075121aa9eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-18T11:49:51.383063Z",
     "iopub.status.busy": "2021-11-18T11:49:51.382238Z",
     "iopub.status.idle": "2021-11-18T11:51:23.199749Z",
     "shell.execute_reply": "2021-11-18T11:51:23.198978Z",
     "shell.execute_reply.started": "2021-11-18T11:49:51.383027Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "predata = DataDel(data_path='data/data_preprocessed_matlab')\n",
    "#import os\n",
    "V,L = predata.getdata('DATA_MSBAM_DEAP_A/')\n",
    "V = V.astype('float32')\n",
    "L = L.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8359a59-d62d-449c-8ebd-25ebfd71d911",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T12:11:20.300331Z",
     "iopub.status.busy": "2021-11-19T12:11:20.299840Z",
     "iopub.status.idle": "2021-11-19T12:11:20.305139Z",
     "shell.execute_reply": "2021-11-19T12:11:20.304407Z",
     "shell.execute_reply.started": "2021-11-19T12:11:20.300288Z"
    }
   },
   "outputs": [],
   "source": [
    "def we():\n",
    "    file = open('what.txt', 'a')\n",
    "    file.write(\"---------results----------\")\n",
    "    file.close\n",
    "we()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea90fe4f-4b49-44d3-9d4c-ea50ba4024b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-18T11:52:26.594897Z",
     "iopub.status.busy": "2021-11-18T11:52:26.594132Z",
     "iopub.status.idle": "2021-11-18T11:52:26.601416Z",
     "shell.execute_reply": "2021-11-18T11:52:26.600681Z",
     "shell.execute_reply.started": "2021-11-18T11:52:26.594860Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960, 320)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(666)\n",
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "for idx_fold, (index_train, index_test) in enumerate(kf.split(V)):\n",
    "    train_index,test_index = index_train,index_test\n",
    "    break\n",
    "len(train_index),len(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86d82478-3ea2-49ec-b62d-eec5820da483",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-18T11:54:09.692060Z",
     "iopub.status.busy": "2021-11-18T11:54:09.691222Z",
     "iopub.status.idle": "2021-11-18T11:54:12.344226Z",
     "shell.execute_reply": "2021-11-18T11:54:12.343405Z",
     "shell.execute_reply.started": "2021-11-18T11:54:09.692026Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11520, 1, 640, 9, 9), (11520,), (3840, 1, 640, 9, 9), (3840,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V,L,data_test,lable_test = proce.prepare_data(train_index,test_index,V,L)\n",
    "V.shape,L.shape,data_test.shape,lable_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e3b28c3-92e5-4e1c-8a77-a7b4825a42b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-18T11:55:05.348862Z",
     "iopub.status.busy": "2021-11-18T11:55:05.347959Z",
     "iopub.status.idle": "2021-11-18T11:55:06.391603Z",
     "shell.execute_reply": "2021-11-18T11:55:06.390411Z",
     "shell.execute_reply.started": "2021-11-18T11:55:05.348821Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10080, 1, 640, 9, 9), (10080,), (1440, 1, 640, 9, 9), (1440,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V,L,val,val_l = proce.split_balance_class(V,L,k=8)\n",
    "V.shape,L.shape,val.shape,val_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5beceaa-738e-4bfa-abe9-60f30fc82eb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-18T11:56:25.671339Z",
     "iopub.status.busy": "2021-11-18T11:56:25.670625Z",
     "iopub.status.idle": "2021-11-18T11:56:25.674129Z",
     "shell.execute_reply": "2021-11-18T11:56:25.673628Z",
     "shell.execute_reply.started": "2021-11-18T11:56:25.671295Z"
    }
   },
   "outputs": [],
   "source": [
    "proce.train(V,L,val,val_l,epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2b09a6f-3bf3-425d-9dc6-90471cc207b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-18T11:57:53.202089Z",
     "iopub.status.busy": "2021-11-18T11:57:53.201336Z",
     "iopub.status.idle": "2021-11-18T11:57:53.540570Z",
     "shell.execute_reply": "2021-11-18T11:57:53.539916Z",
     "shell.execute_reply.started": "2021-11-18T11:57:53.202052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Test:  loss=1.7171 acc=0.5910 f1=0.6794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5909722222222222, 0.6793685356559608)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act,pred,loss = proce.test(val,val_l,'models-MSBAM/model_epoch_10.pdparams')\n",
    "acc , f1 ,_ = proce.get_metrics(act,pred)\n",
    "#a,b = trial_wise_voting(act,pred,num_segment_per_trial=12)\n",
    "#acc , f1 ,_ = get_metrics(a,b)\n",
    "acc,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a505bb0-2cc9-4839-a4d9-5f80d4eef19e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-18T12:41:05.181051Z",
     "iopub.status.busy": "2021-11-18T12:41:05.180242Z",
     "iopub.status.idle": "2021-11-18T12:41:05.184130Z",
     "shell.execute_reply": "2021-11-18T12:41:05.183567Z",
     "shell.execute_reply.started": "2021-11-18T12:41:05.181016Z"
    }
   },
   "outputs": [],
   "source": [
    "path = '32'+'/model_{}_fold'.format(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e52787c8-1a15-41ce-a458-fe481e1c3ade",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-18T12:41:08.039658Z",
     "iopub.status.busy": "2021-11-18T12:41:08.038588Z",
     "iopub.status.idle": "2021-11-18T12:41:08.043360Z",
     "shell.execute_reply": "2021-11-18T12:41:08.042847Z",
     "shell.execute_reply.started": "2021-11-18T12:41:08.039614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'32/model_3_fold'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b5292a-ac93-4fa8-9b41-8927f813cd2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
